{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e803ba-d25b-496d-83a3-00af25283942",
   "metadata": {},
   "source": [
    "#  Modeling and Evaluation\n",
    "\n",
    "In this notebook, we will train and evaluate baseline machine learning models for both:\n",
    "\n",
    "- **Revenue Prediction** (Regression)\n",
    "- **Success Classification** (Binary Classification)\n",
    "\n",
    "We will use Scikit-learn pipelines to ensure clean preprocessing, scaling, and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc14b807-1e6f-4fc6-b9be-0ed41eb87627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01a9f72-d41c-4385-8b5a-1ef30f7fc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(\"../data/clean_movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa95844-cbd4-4f6c-82e1-a09db57ece3f",
   "metadata": {},
   "source": [
    "##  Training Baseline Models with Pipelines\n",
    "\n",
    "We now create baseline models for both regression and classification tasks using Scikit-learn Pipelines.\n",
    "\n",
    "- **Regression:** RandomForestRegressor\n",
    "- **Classification:** LogisticRegression\n",
    "\n",
    "Each pipeline includes a StandardScaler to normalize the features before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34aa96a1-fd93-4418-b6e3-fe5298d4f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = movies_df.drop(columns=['revenue', 'success'])\n",
    "y_reg = movies_df['revenue']\n",
    "y_clf = movies_df['success']\n",
    "\n",
    " \n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    " \n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X, y_clf, test_size=0.2, stratify=y_clf, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02a75af-ab49-4622-8222-3c085da7d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "reg_pipeline.fit(X_train_reg, y_train_reg)\n",
    "\n",
    " \n",
    "y_pred_reg = reg_pipeline.predict(X_test_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79dd4188-9f06-42fd-afd1-67b39fb3c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "clf_pipeline.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "y_pred_clf = clf_pipeline.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cc9cd-8361-4538-bad8-5bf725fd3a00",
   "metadata": {},
   "source": [
    "##  Model Evaluation\n",
    "\n",
    "In this step, we evaluate the performance of our baseline models using appropriate metrics.\n",
    "\n",
    "- For **regression**, we assess:  \n",
    "  - MAE (Mean Absolute Error)  \n",
    "  - RMSE (Root Mean Squared Error)  \n",
    "  - R¬≤ Score (Explained Variance)\n",
    "\n",
    "- For **classification**, we assess:  \n",
    "  - Accuracy  \n",
    "  - Precision  \n",
    "  - Recall  \n",
    "  - F1 Score  \n",
    "  - ROC AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ac4db01-b6c0-49d7-bcd3-835e9c63d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Metrics:\n",
      "MAE  = 37,690,098.67\n",
      "RMSE = 75,519,375.03\n",
      "R¬≤   = 0.7807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"Regression Metrics:\")\n",
    "print(f\"MAE  = {mae:,.2f}\")\n",
    "print(f\"RMSE = {rmse:,.2f}\")\n",
    "print(f\"R¬≤   = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d149b101-78e3-475e-bb0b-3dcf2cc7578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Not Successful       0.68      0.57      0.62       392\n",
      "    Successful       0.73      0.82      0.77       569\n",
      "\n",
      "      accuracy                           0.71       961\n",
      "     macro avg       0.71      0.69      0.69       961\n",
      "  weighted avg       0.71      0.71      0.71       961\n",
      "\n",
      "ROC AUC: 0.7830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "print(classification_report(y_test_clf, y_pred_clf, target_names=[\"Not Successful\", \"Successful\"]))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_clf, clf_pipeline.predict_proba(X_test_clf)[:,1])\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a0454-e4cd-4320-bd75-8c669eb9c4b3",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "We will now use `GridSearchCV` to tune our models and find the best hyperparameters for:\n",
    "\n",
    "- **LogisticRegression** (for classification)\n",
    "- **RandomForestRegressor** (for regression)\n",
    "\n",
    "We use 5-fold cross-validation to ensure our results are reliable and generalizable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5833a05-70c5-4e55-a0bd-5883dd4039d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Params: {'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_logreg = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__solver': ['lbfgs']\n",
    "}\n",
    "grid_clf = GridSearchCV(clf_pipeline, param_grid_logreg, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "grid_clf.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "print(\"Best Logistic Regression Params:\", grid_clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fcc154d-60c3-46f2-8637-7569b38fe517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Params: {'model__max_depth': 10, 'model__min_samples_split': 5, 'model__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    " \n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20, None],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "}\n",
    " \n",
    "grid_reg = GridSearchCV(reg_pipeline, param_grid_rf, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    " \n",
    "grid_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    " \n",
    "print(\"Best Random Forest Params:\", grid_reg.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef8bbe-5044-40c7-a605-ae989a7c56ec",
   "metadata": {},
   "source": [
    "## Evaluation of Tuned Models\n",
    "\n",
    "Now that we have identified the best hyperparameters using GridSearchCV, we evaluate the performance of the tuned models on the test set.\n",
    "\n",
    "We compare the results with our baseline models to determine whether tuning improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e9a62ea-533d-407c-b0ac-5c737f4ac4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Regression Model Metrics:\n",
      "MAE  = 37,148,816.07\n",
      "RMSE = 75,969,538.58\n",
      "R¬≤   = 0.7780\n"
     ]
    }
   ],
   "source": [
    "best_reg = grid_reg.best_estimator_\n",
    "\n",
    "y_pred_reg_tuned = best_reg.predict(X_test_reg)\n",
    "\n",
    "mae_tuned = mean_absolute_error(y_test_reg, y_pred_reg_tuned)\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_tuned))\n",
    "r2_tuned = r2_score(y_test_reg, y_pred_reg_tuned)\n",
    "\n",
    "print(\"Tuned Regression Model Metrics:\")\n",
    "print(f\"MAE  = {mae_tuned:,.2f}\")\n",
    "print(f\"RMSE = {rmse_tuned:,.2f}\")\n",
    "print(f\"R¬≤   = {r2_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42fc202a-3785-48dc-a36a-424c9ca51385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Classification Model Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Not Successful       0.68      0.57      0.62       392\n",
      "    Successful       0.73      0.82      0.77       569\n",
      "\n",
      "      accuracy                           0.72       961\n",
      "     macro avg       0.71      0.69      0.70       961\n",
      "  weighted avg       0.71      0.72      0.71       961\n",
      "\n",
      "ROC AUC: 0.7835\n"
     ]
    }
   ],
   "source": [
    "best_clf = grid_clf.best_estimator_\n",
    "\n",
    "y_pred_clf_tuned = best_clf.predict(X_test_clf)\n",
    "\n",
    "print(\"Tuned Classification Model Report:\")\n",
    "print(classification_report(y_test_clf, y_pred_clf_tuned, target_names=[\"Not Successful\", \"Successful\"]))\n",
    "\n",
    " \n",
    "roc_auc_tuned = roc_auc_score(y_test_clf, best_clf.predict_proba(X_test_clf)[:,1])\n",
    "print(f\"ROC AUC: {roc_auc_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e42a7947-42f5-4a21-9d44-4177eefcd1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2712f2e-90b8-4240-8118-54c3096cb0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model saved as classification_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "joblib.dump(grid_clf.best_estimator_, \"../models/classification_model.pkl\")\n",
    "print(\"Classification model saved as classification_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "082459b1-5249-4301-82dc-3dfe9572425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression model saved as regression_model.pkl\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(grid_reg.best_estimator_, \"../models/regression_model.pkl\")\n",
    "print(\"Regression model saved as regression_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be808d48-1d0f-4c0d-9152-d4ea1c3a59e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/test_data_regression.pkl']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((X_test_reg, y_test_reg), \"../models/test_data_regression.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "108cc573-3a11-423a-93f5-2df6263527ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/test_data_clf.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((X_test_clf, y_test_clf), \"../models/test_data_clf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8126485-54a6-441f-be16-ffc1bb990465",
   "metadata": {},
   "source": [
    "## Summary of Modeling\n",
    "\n",
    "Both regression and classification pipelines were trained and tuned using GridSearchCV.\n",
    "\n",
    "- **Best classification model:** Logistic Regression with C=10\n",
    "- **Best regression model:** Random Forest with 200 trees, max_depth=10\n",
    "\n",
    "We now proceed to visualize and interpret the results in the next notebook:  \n",
    "üìÅ `03_evaluation.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98ddae-37f4-4efe-986c-86a5ec852784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
